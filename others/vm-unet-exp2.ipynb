{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13454629,"sourceType":"datasetVersion","datasetId":8540409},{"sourceId":13454783,"sourceType":"datasetVersion","datasetId":8540507}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms.functional as TF\nimport random\n\n# Data paths\ndata_path = r\"/kaggle/input/monusegdataset/data\"\n\n# ============================================================================\n# DATASET PROCESSING\n# ============================================================================\n\n# Data Augmentation Class\nclass NucleiAugmentation:\n    def __init__(self, image_size=256, augmentation_prob=0.7, sparse_mode=False):\n        self.image_size = image_size\n        self.augmentation_prob = augmentation_prob\n        self.sparse_mode = sparse_mode\n    \n    def __call__(self, image, mask):\n        if torch.is_tensor(image):\n            image = TF.to_pil_image(image)\n        if torch.is_tensor(mask):\n            mask = TF.to_pil_image(mask)\n        \n        # Random horizontal flip\n        if random.random() > self.augmentation_prob:\n            image = TF.hflip(image)\n            mask = TF.hflip(mask)\n        \n        # Random vertical flip\n        if random.random() > self.augmentation_prob:\n            image = TF.vflip(image)\n            mask = TF.vflip(mask)\n        \n        # Random rotation (-45 to +45 degrees)\n        if random.random() > self.augmentation_prob:\n            angle = random.uniform(-45, 45)\n            image = TF.rotate(image, angle)\n            mask = TF.rotate(mask, angle)\n        \n        # Random brightness/contrast adjustment\n        if random.random() > self.augmentation_prob:\n            brightness_factor = random.uniform(0.8, 1.2)\n            image = TF.adjust_brightness(image, brightness_factor)\n        \n        if random.random() > self.augmentation_prob:\n            contrast_factor = random.uniform(0.8, 1.2)\n            image = TF.adjust_contrast(image, contrast_factor)\n        \n        # Random Gaussian blur\n        if random.random() > self.augmentation_prob:\n            image = TF.gaussian_blur(image, kernel_size=3)\n        \n        # For sparse mode: random crop\n        if self.sparse_mode and random.random() > 0.8:\n            width, height = image.size\n            crop_size = random.randint(128, 256)\n            i = random.randint(0, height - crop_size)\n            j = random.randint(0, width - crop_size)\n            image = TF.crop(image, i, j, crop_size, crop_size)\n            mask = TF.crop(mask, i, j, crop_size, crop_size)\n        \n        # Ensure final size\n        image = image.resize((self.image_size, self.image_size))\n        mask = mask.resize((self.image_size, self.image_size))\n        \n        return image, mask\n\n# Dataset Class\nclass KMMSDataset(Dataset):\n    def __init__(self, image_paths, mask_paths, image_size=256, augment=False, sparse_mode=False):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.image_size = image_size\n        self.augment = augment\n        self.sparse_mode = sparse_mode\n        self.augmentor = NucleiAugmentation(image_size, sparse_mode=sparse_mode) if augment else None\n        \n        self.valid_pairs = list(zip(image_paths, mask_paths))\n        \n        print(f\"Created dataset with {len(self.valid_pairs)} valid image-mask pairs\")\n        if augment:\n            mode = \"sparse\" if sparse_mode else \"normal\"\n            print(f\"âœ“ Data augmentation ENABLED ({mode} mode)\")\n        else:\n            print(\"âœ— Data augmentation DISABLED\")\n    \n    def __len__(self):\n        return len(self.valid_pairs)\n    \n    def __getitem__(self, idx):\n        img_path, mask_path = self.valid_pairs[idx]\n        \n        try:\n            # Load image\n            image = Image.open(img_path).convert('RGB')\n            mask = Image.open(mask_path).convert('L')\n            \n            # Apply augmentation\n            if self.augment and self.augmentor:\n                image, mask = self.augmentor(image, mask)\n            else:\n                image = image.resize((self.image_size, self.image_size))\n                mask = mask.resize((self.image_size, self.image_size))\n            \n            # Convert to numpy arrays\n            image = np.array(image).astype(np.float32) / 255.0\n            mask = np.array(mask).astype(np.float32) / 255.0\n            \n            # Ensure mask is binary\n            mask = (mask > 0.5).astype(np.float32)\n            \n            # Convert to tensors\n            image = torch.from_numpy(image).permute(2, 0, 1).float()\n            mask = torch.from_numpy(mask).unsqueeze(0).float()\n            \n            return image, mask, os.path.basename(img_path)\n        \n        except Exception as e:\n            print(f\"Error loading {img_path}: {e}\")\n            dummy_image = torch.zeros(3, self.image_size, self.image_size)\n            dummy_mask = torch.zeros(1, self.image_size, self.image_size)\n            return dummy_image, dummy_mask, \"error\"\n\n# Data loading and balancing\ndef find_kmms_data():\n    train_path = os.path.join(data_path, \"kmms_training\")\n    test_path = os.path.join(data_path, \"kmms_test\")\n    \n    train_images, train_masks, test_images, test_masks = [], [], [], []\n    \n    # Load training data\n    if os.path.exists(train_path):\n        train_images_dir = os.path.join(train_path, \"images\")\n        train_masks_dir = os.path.join(train_path, \"masks\")\n        if os.path.exists(train_images_dir):\n            train_images = sorted([os.path.join(train_images_dir, f) for f in os.listdir(train_images_dir) \n                                 if f.lower().endswith(('.tiff', '.tif', '.png', '.jpg', '.jpeg'))])\n        if os.path.exists(train_masks_dir):\n            train_masks = sorted([os.path.join(train_masks_dir, f) for f in os.listdir(train_masks_dir) \n                                if f.lower().endswith(('.tiff', '.tif', '.png', '.jpg', '.jpeg'))])\n    \n    # Load test data\n    if os.path.exists(test_path):\n        test_images_dir = os.path.join(test_path, \"images\")\n        test_masks_dir = os.path.join(test_path, \"masks\")\n        if os.path.exists(test_images_dir):\n            test_images = sorted([os.path.join(test_images_dir, f) for f in os.listdir(test_images_dir) \n                                if f.lower().endswith(('.tiff', '.tif', '.png', '.jpg', '.jpeg'))])\n        if os.path.exists(test_masks_dir):\n            test_masks = sorted([os.path.join(test_masks_dir, f) for f in os.listdir(test_masks_dir) \n                               if f.lower().endswith(('.tiff', '.tif', '.png', '.jpg', '.jpeg'))])\n    \n    print(f\"Found: {len(train_images)} training images, {len(test_images)} test images\")\n    return train_images, train_masks, test_images, test_masks\n\n# Improved data splitting function\ndef create_balanced_split(all_images, all_masks, test_size=0.15, val_size=0.15, random_state=42):\n    \"\"\"\n    Create a balanced train/val/test split from all available data\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n    \n    # First split: Train+Val vs Test\n    train_val_images, test_images, train_val_masks, test_masks = train_test_split(\n        all_images, all_masks, test_size=test_size, random_state=random_state\n    )\n    \n    # Second split: Train vs Val from train_val set\n    val_ratio = val_size / (1 - test_size)\n    train_images, val_images, train_masks, val_masks = train_test_split(\n        train_val_images, train_val_masks, test_size=val_ratio, random_state=random_state\n    )\n    \n    total_samples = len(all_images)\n    print(f\"âœ… Balanced Data Split:\")\n    print(f\"   Training: {len(train_images)} images ({len(train_images)/total_samples*100:.1f}%)\")\n    print(f\"   Validation: {len(val_images)} images ({len(val_images)/total_samples*100:.1f}%)\")\n    print(f\"   Test: {len(test_images)} images ({len(test_images)/total_samples*100:.1f}%)\")\n    \n    return train_images, val_images, test_images, train_masks, val_masks, test_masks\n\n# Main dataset preparation function\ndef prepare_datasets(image_size=256, batch_size=4, balanced_split=True):\n    \"\"\"\n    Main function to prepare datasets with optional balanced splitting\n    \"\"\"\n    # Load all data\n    train_images, train_masks, test_images, test_masks = find_kmms_data()\n    \n    if balanced_split:\n        # Combine all data for balanced splitting\n        all_images = train_images + test_images\n        all_masks = train_masks + test_masks\n        \n        # Create balanced split (70-15-15 recommended)\n        train_images, val_images, test_images, train_masks, val_masks, test_masks = create_balanced_split(\n            all_images, all_masks, test_size=0.15, val_size=0.15\n        )\n    else:\n        # Use original split (80-20 from training data for validation)\n        from sklearn.model_selection import train_test_split\n        train_images, val_images, train_masks, val_masks = train_test_split(\n            train_images, train_masks, test_size=0.2, random_state=42\n        )\n        print(f\"Using original split: Train={len(train_images)}, Val={len(val_images)}, Test={len(test_images)}\")\n    \n    # Create datasets\n    train_dataset = KMMSDataset(train_images, train_masks, image_size=image_size, augment=True, sparse_mode=True)\n    val_dataset = KMMSDataset(val_images, val_masks, image_size=image_size, augment=False, sparse_mode=False)\n    test_dataset = KMMSDataset(test_images, test_masks, image_size=image_size, augment=False, sparse_mode=False)\n    \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    \n    return train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset\n\n# Example usage\nif __name__ == \"__main__\":\n    # Prepare datasets with balanced splitting (recommended)\n    train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = prepare_datasets(\n        image_size=256, \n        batch_size=4, \n        balanced_split=True  # Set to False to use original split\n    )\n    \n    print(f\"\\nFinal dataset sizes:\")\n    print(f\"Training: {len(train_dataset)} samples\")\n    print(f\"Validation: {len(val_dataset)} samples\") \n    print(f\"Test: {len(test_dataset)} samples\")\n    \n    # Test data loading\n    print(f\"\\nTesting data loader...\")\n    for images, masks, filenames in train_loader:\n        print(f\"Batch - Images: {images.shape}, Masks: {masks.shape}\")\n        break","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-21T13:45:43.606886Z","iopub.execute_input":"2025-10-21T13:45:43.607733Z","iopub.status.idle":"2025-10-21T13:45:44.243989Z","shell.execute_reply.started":"2025-10-21T13:45:43.607699Z","shell.execute_reply":"2025-10-21T13:45:44.243259Z"}},"outputs":[{"name":"stdout","text":"Found: 24 training images, 58 test images\nâœ… Balanced Data Split:\n   Training: 56 images (68.3%)\n   Validation: 13 images (15.9%)\n   Test: 13 images (15.9%)\nCreated dataset with 56 valid image-mask pairs\nâœ“ Data augmentation ENABLED (sparse mode)\nCreated dataset with 13 valid image-mask pairs\nâœ— Data augmentation DISABLED\nCreated dataset with 13 valid image-mask pairs\nâœ— Data augmentation DISABLED\n\nFinal dataset sizes:\nTraining: 56 samples\nValidation: 13 samples\nTest: 13 samples\n\nTesting data loader...\nBatch - Images: torch.Size([4, 3, 256, 256]), Masks: torch.Size([4, 1, 256, 256])\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ============================================================================\n# FIXED VM-UNET MODEL ARCHITECTURE\n# ============================================================================\n\nclass VMBlock(nn.Module):\n    \"\"\"Vision Mamba Block with residual connection\"\"\"\n    def __init__(self, channels):\n        super().__init__()\n        self.norm = nn.LayerNorm(channels)\n        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.activation = nn.GELU()\n        \n    def forward(self, x):\n        residual = x\n        # LayerNorm and channel-first for conv\n        x = x.permute(0, 2, 3, 1)  # [B, C, H, W] -> [B, H, W, C]\n        x = self.norm(x)\n        x = x.permute(0, 3, 1, 2)  # [B, H, W, C] -> [B, C, H, W]\n        \n        x = self.conv1(x)\n        x = self.activation(x)\n        x = self.conv2(x)\n        \n        return x + residual\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.GELU(),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.GELU()\n        )\n    \n    def forward(self, x):\n        return self.double_conv(x)\n\nclass VMEncoderBlock(nn.Module):\n    \"\"\"Encoder block with VMBlock\"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = DoubleConv(in_channels, out_channels)\n        self.vm_block = VMBlock(out_channels)\n        self.pool = nn.MaxPool2d(2)\n    \n    def forward(self, x):\n        # Feature extraction\n        x = self.conv(x)\n        # Mamba-style processing\n        x = self.vm_block(x)\n        # Downsample\n        pooled = self.pool(x)\n        return pooled, x\n\nclass VMDecoderBlock(nn.Module):\n    \"\"\"FIXED Decoder block with proper channel handling\"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        # Upsample reduces channels by 2\n        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n        # After concatenation: (in_channels//2 + skip_channels) -> out_channels\n        self.conv = DoubleConv(in_channels // 2 + out_channels, out_channels)  # FIXED\n        self.vm_block = VMBlock(out_channels)\n    \n    def forward(self, x, skip):\n        # Upsample\n        x = self.up(x)\n        # Skip connection - ensure spatial dimensions match\n        if x.shape[2:] != skip.shape[2:]:\n            x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=True)\n        \n        # Concatenate along channel dimension\n        x = torch.cat([x, skip], dim=1)\n        # Feature extraction\n        x = self.conv(x)\n        # Mamba-style processing\n        x = self.vm_block(x)\n        return x\n\nclass VMUNet(nn.Module):\n    def __init__(self, n_channels=3, n_classes=1):\n        super().__init__()\n        \n        # Encoder path\n        self.enc1 = VMEncoderBlock(n_channels, 64)    # 64 channels\n        self.enc2 = VMEncoderBlock(64, 128)           # 128 channels  \n        self.enc3 = VMEncoderBlock(128, 256)          # 256 channels\n        self.enc4 = VMEncoderBlock(256, 512)          # 512 channels\n        \n        # Bridge\n        self.bridge = DoubleConv(512, 1024)           # 1024 channels\n        \n        # Decoder path - FIXED channel dimensions\n        self.dec1 = VMDecoderBlock(1024, 512)         # 1024->512 after up, then 512+512=1024->512\n        self.dec2 = VMDecoderBlock(512, 256)          # 512->256 after up, then 256+256=512->256\n        self.dec3 = VMDecoderBlock(256, 128)          # 256->128 after up, then 128+128=256->128\n        self.dec4 = VMDecoderBlock(128, 64)           # 128->64 after up, then 64+64=128->64\n        \n        # Output\n        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n        \n    def forward(self, x):\n        # Encoder\n        x1, skip1 = self.enc1(x)    # skip1: 64 channels\n        x2, skip2 = self.enc2(x1)   # skip2: 128 channels\n        x3, skip3 = self.enc3(x2)   # skip3: 256 channels\n        x4, skip4 = self.enc4(x3)   # skip4: 512 channels\n        \n        # Bridge\n        x5 = self.bridge(x4)        # 1024 channels\n        \n        # Decoder with proper skip connections\n        x = self.dec1(x5, skip4)    # 512 channels\n        x = self.dec2(x, skip3)     # 256 channels\n        x = self.dec3(x, skip2)     # 128 channels\n        x = self.dec4(x, skip1)     # 64 channels\n        \n        # Output\n        return torch.sigmoid(self.outc(x))\n\n# ============================================================================\n# SIMPLIFIED ENHANCED VERSION (Easier to debug)\n# ============================================================================\n\nclass SimpleVMUNet(nn.Module):\n    \"\"\"Simplified version that's easier to debug\"\"\"\n    def __init__(self, n_channels=3, n_classes=1):\n        super().__init__()\n        \n        # Encoder\n        self.enc1 = DoubleConv(n_channels, 64)\n        self.pool1 = nn.MaxPool2d(2)\n        self.enc2 = DoubleConv(64, 128)\n        self.pool2 = nn.MaxPool2d(2)\n        self.enc3 = DoubleConv(128, 256)\n        self.pool3 = nn.MaxPool2d(2)\n        self.enc4 = DoubleConv(256, 512)\n        self.pool4 = nn.MaxPool2d(2)\n        \n        # Bridge with VMBlock\n        self.bridge = DoubleConv(512, 1024)\n        self.vm_bridge = VMBlock(1024)\n        \n        # Decoder\n        self.up1 = nn.ConvTranspose2d(1024, 512, 2, 2)\n        self.dec1 = DoubleConv(1024, 512)  # 512 (up) + 512 (skip)\n        self.vm1 = VMBlock(512)\n        \n        self.up2 = nn.ConvTranspose2d(512, 256, 2, 2)\n        self.dec2 = DoubleConv(512, 256)   # 256 (up) + 256 (skip)\n        self.vm2 = VMBlock(256)\n        \n        self.up3 = nn.ConvTranspose2d(256, 128, 2, 2)\n        self.dec3 = DoubleConv(256, 128)   # 128 (up) + 128 (skip)\n        self.vm3 = VMBlock(128)\n        \n        self.up4 = nn.ConvTranspose2d(128, 64, 2, 2)\n        self.dec4 = DoubleConv(128, 64)    # 64 (up) + 64 (skip)\n        self.vm4 = VMBlock(64)\n        \n        self.outc = nn.Conv2d(64, n_classes, 1)\n        \n    def forward(self, x):\n        # Encoder\n        e1 = self.enc1(x)\n        e2 = self.enc2(self.pool1(e1))\n        e3 = self.enc3(self.pool2(e2))\n        e4 = self.enc4(self.pool3(e3))\n        \n        # Bridge\n        b = self.pool4(e4)\n        b = self.bridge(b)\n        b = self.vm_bridge(b)\n        \n        # Decoder\n        d1 = self.up1(b)\n        d1 = torch.cat([d1, e4], dim=1)\n        d1 = self.dec1(d1)\n        d1 = self.vm1(d1)\n        \n        d2 = self.up2(d1)\n        d2 = torch.cat([d2, e3], dim=1)\n        d2 = self.dec2(d2)\n        d2 = self.vm2(d2)\n        \n        d3 = self.up3(d2)\n        d3 = torch.cat([d3, e2], dim=1)\n        d3 = self.dec3(d3)\n        d3 = self.vm3(d3)\n        \n        d4 = self.up4(d3)\n        d4 = torch.cat([d4, e1], dim=1)\n        d4 = self.dec4(d4)\n        d4 = self.vm4(d4)\n        \n        return torch.sigmoid(self.outc(d4))\n\n# ============================================================================\n# TEST THE MODEL WITH YOUR DATA\n# ============================================================================\n\ndef test_model_with_data():\n    \"\"\"Test the model with your data loader to verify it works\"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Testing on device: {device}\")\n    \n    # Load your data\n    train_loader, val_loader, test_loader, _, _, _ = prepare_datasets(\n        image_size=256, \n        batch_size=4, \n        balanced_split=True\n    )\n    \n    # Test both models\n    models = {\n        \"Fixed VM-UNet\": VMUNet(n_channels=3, n_classes=1),\n        \"Simple VM-UNet\": SimpleVMUNet(n_channels=3, n_classes=1)\n    }\n    \n    for name, model in models.items():\n        print(f\"\\n{'='*50}\")\n        print(f\"Testing {name}\")\n        print(f\"{'='*50}\")\n        \n        model.to(device)\n        \n        # Test with a batch\n        try:\n            with torch.no_grad():\n                for images, masks, _ in train_loader:\n                    images = images.to(device)\n                    print(f\"Input shape: {images.shape}\")\n                    \n                    outputs = model(images)\n                    print(f\"Output shape: {outputs.shape}\")\n                    print(f\"âœ… {name} works correctly!\")\n                    break\n                    \n        except Exception as e:\n            print(f\"âŒ Error in {name}: {e}\")\n    \n    return models\n\n# ============================================================================\n# UPDATED TRAINING FUNCTION\n# ============================================================================\n\ndef train_fixed_vm_unet(model, train_loader, val_loader, num_epochs=50, device='cuda'):\n    model.to(device)\n    \n    criterion = CombinedLoss(alpha=0.7, beta=0.3)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n    \n    train_losses = []\n    val_losses = []\n    train_f1_scores = []\n    val_f1_scores = []\n    \n    best_val_f1 = 0.0\n    best_model_state = None\n    \n    print(\"ðŸš€ Starting Fixed VM-UNet Training...\")\n    \n    for epoch in range(num_epochs):\n        # Training\n        model.train()\n        epoch_train_loss = 0.0\n        train_preds = []\n        train_targets = []\n        \n        for images, masks, _ in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]'):\n            images = images.to(device)\n            masks = masks.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            loss.backward()\n            optimizer.step()\n            \n            epoch_train_loss += loss.item()\n            train_preds.append(outputs.detach())\n            train_targets.append(masks.detach())\n        \n        # Validation\n        model.eval()\n        epoch_val_loss = 0.0\n        val_preds = []\n        val_targets = []\n        \n        with torch.no_grad():\n            for images, masks, _ in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]'):\n                images = images.to(device)\n                masks = masks.to(device)\n                \n                outputs = model(images)\n                loss = criterion(outputs, masks)\n                epoch_val_loss += loss.item()\n                \n                val_preds.append(outputs)\n                val_targets.append(masks)\n        \n        # Calculate metrics\n        avg_train_loss = epoch_train_loss / len(train_loader)\n        avg_val_loss = epoch_val_loss / len(val_loader)\n        \n        train_preds = torch.cat(train_preds)\n        train_targets = torch.cat(train_targets)\n        train_precision, train_recall, train_accuracy, train_f1 = calculate_metrics(train_preds, train_targets, threshold=0.3)\n        \n        val_preds = torch.cat(val_preds)\n        val_targets = torch.cat(val_targets)\n        val_precision, val_recall, val_accuracy, val_f1 = calculate_metrics(val_preds, val_targets, threshold=0.3)\n        \n        # Store results\n        train_losses.append(avg_train_loss)\n        val_losses.append(avg_val_loss)\n        train_f1_scores.append(train_f1)\n        val_f1_scores.append(val_f1)\n        \n        scheduler.step(avg_val_loss)\n        \n        print(f'\\nðŸ“ˆ Epoch {epoch+1}/{num_epochs}:')\n        print(f'   Train Loss: {avg_train_loss:.4f} | Prec: {train_precision:.4f} | Rec: {train_recall:.4f} | F1: {train_f1:.4f}')\n        print(f'   Val Loss: {avg_val_loss:.4f} | Prec: {val_precision:.4f} | Rec: {val_recall:.4f} | F1: {val_f1:.4f}')\n        \n        # Save best model\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            best_model_state = model.state_dict().copy()\n            torch.save(best_model_state, 'best_fixed_vm_unet_model.pth')\n            print(f'ðŸ’¾ New best model saved! Val F1: {best_val_f1:.4f}')\n    \n    if best_model_state is not None:\n        model.load_state_dict(best_model_state)\n    return model, train_losses, val_losses, train_f1_scores, val_f1_scores\n\n# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\n\ndef main():\n    # First test the models\n    models = test_model_with_data()\n    \n    # Use the simple model for training (more stable)\n    model = models[\"Simple VM-UNet\"]\n    \n    # Load datasets\n    train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = prepare_datasets(\n        image_size=256, \n        batch_size=4, \n        balanced_split=True\n    )\n    \n    print(f\"\\nðŸ“Š Dataset sizes: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n    \n    # Train the model\n    print(\"\\nðŸŽ¯ Starting training...\")\n    model, train_losses, val_losses, train_f1_scores, val_f1_scores = train_fixed_vm_unet(\n        model, train_loader, val_loader, num_epochs=50, device='cuda'\n    )\n    \n    # Test the model\n    test_metrics = test_vm_unet(model, test_loader, device='cuda')\n    \n    # Save final model\n    torch.save(model.state_dict(), 'final_fixed_vm_unet_model.pth')\n    print(\"\\nðŸ’¾ Final model saved!\")\n    \n    return model, test_metrics\n\nif __name__ == \"__main__\":\n    model, metrics = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T14:12:42.055733Z","iopub.execute_input":"2025-10-21T14:12:42.056522Z","iopub.status.idle":"2025-10-21T14:28:26.689575Z","shell.execute_reply.started":"2025-10-21T14:12:42.056491Z","shell.execute_reply":"2025-10-21T14:28:26.688653Z"}},"outputs":[{"name":"stdout","text":"Testing on device: cuda\nFound: 24 training images, 58 test images\nâœ… Balanced Data Split:\n   Training: 56 images (68.3%)\n   Validation: 13 images (15.9%)\n   Test: 13 images (15.9%)\nCreated dataset with 56 valid image-mask pairs\nâœ“ Data augmentation ENABLED (sparse mode)\nCreated dataset with 13 valid image-mask pairs\nâœ— Data augmentation DISABLED\nCreated dataset with 13 valid image-mask pairs\nâœ— Data augmentation DISABLED\n\n==================================================\nTesting Fixed VM-UNet\n==================================================\nInput shape: torch.Size([4, 3, 256, 256])\nOutput shape: torch.Size([4, 1, 256, 256])\nâœ… Fixed VM-UNet works correctly!\n\n==================================================\nTesting Simple VM-UNet\n==================================================\nInput shape: torch.Size([4, 3, 256, 256])\nOutput shape: torch.Size([4, 1, 256, 256])\nâœ… Simple VM-UNet works correctly!\nFound: 24 training images, 58 test images\nâœ… Balanced Data Split:\n   Training: 56 images (68.3%)\n   Validation: 13 images (15.9%)\n   Test: 13 images (15.9%)\nCreated dataset with 56 valid image-mask pairs\nâœ“ Data augmentation ENABLED (sparse mode)\nCreated dataset with 13 valid image-mask pairs\nâœ— Data augmentation DISABLED\nCreated dataset with 13 valid image-mask pairs\nâœ— Data augmentation DISABLED\n\nðŸ“Š Dataset sizes: Train=56, Val=13, Test=13\n\nðŸŽ¯ Starting training...\nðŸš€ Starting Fixed VM-UNet Training...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.95it/s]\nEpoch 1/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 1/50:\n   Train Loss: 0.4892 | Prec: 0.1542 | Rec: 0.9865 | F1: 0.2667\n   Val Loss: 0.5331 | Prec: 0.0911 | Rec: 0.0056 | F1: 0.0105\nðŸ’¾ New best model saved! Val F1: 0.0105\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.91it/s]\nEpoch 2/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 2/50:\n   Train Loss: 0.3721 | Prec: 0.3575 | Rec: 0.8770 | F1: 0.5079\n   Val Loss: 0.6211 | Prec: 0.8502 | Rec: 0.1170 | F1: 0.2058\nðŸ’¾ New best model saved! Val F1: 0.2058\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.90it/s]\nEpoch 3/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  4.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 3/50:\n   Train Loss: 0.3209 | Prec: 0.5388 | Rec: 0.7533 | F1: 0.6283\n   Val Loss: 0.3588 | Prec: 0.6709 | Rec: 0.5795 | F1: 0.6218\nðŸ’¾ New best model saved! Val F1: 0.6218\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:06<00:00,  2.04it/s]\nEpoch 4/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 4/50:\n   Train Loss: 0.2892 | Prec: 0.5858 | Rec: 0.7652 | F1: 0.6636\n   Val Loss: 0.3137 | Prec: 0.6830 | Rec: 0.6315 | F1: 0.6562\nðŸ’¾ New best model saved! Val F1: 0.6562\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:06<00:00,  2.02it/s]\nEpoch 5/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 5/50:\n   Train Loss: 0.3177 | Prec: 0.5361 | Rec: 0.7772 | F1: 0.6345\n   Val Loss: 0.2893 | Prec: 0.5336 | Rec: 0.8384 | F1: 0.6521\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:06<00:00,  2.00it/s]\nEpoch 6/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 6/50:\n   Train Loss: 0.2749 | Prec: 0.6149 | Rec: 0.7672 | F1: 0.6827\n   Val Loss: 0.2801 | Prec: 0.6690 | Rec: 0.7137 | F1: 0.6906\nðŸ’¾ New best model saved! Val F1: 0.6906\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.99it/s]\nEpoch 7/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 7/50:\n   Train Loss: 0.2476 | Prec: 0.6698 | Rec: 0.7889 | F1: 0.7245\n   Val Loss: 0.2672 | Prec: 0.6770 | Rec: 0.7464 | F1: 0.7100\nðŸ’¾ New best model saved! Val F1: 0.7100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.93it/s]\nEpoch 8/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 8/50:\n   Train Loss: 0.2627 | Prec: 0.5652 | Rec: 0.8381 | F1: 0.6751\n   Val Loss: 0.3039 | Prec: 0.5126 | Rec: 0.8269 | F1: 0.6329\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:06<00:00,  2.04it/s]\nEpoch 9/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 9/50:\n   Train Loss: 0.2614 | Prec: 0.6273 | Rec: 0.7838 | F1: 0.6969\n   Val Loss: 0.2822 | Prec: 0.6863 | Rec: 0.6899 | F1: 0.6881\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  2.00it/s]\nEpoch 10/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 10/50:\n   Train Loss: 0.2419 | Prec: 0.6374 | Rec: 0.8168 | F1: 0.7160\n   Val Loss: 0.2506 | Prec: 0.6237 | Rec: 0.8287 | F1: 0.7118\nðŸ’¾ New best model saved! Val F1: 0.7118\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.99it/s]\nEpoch 11/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 11/50:\n   Train Loss: 0.2542 | Prec: 0.6312 | Rec: 0.7939 | F1: 0.7033\n   Val Loss: 0.2652 | Prec: 0.6382 | Rec: 0.7626 | F1: 0.6949\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:06<00:00,  2.01it/s]\nEpoch 12/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 12/50:\n   Train Loss: 0.2614 | Prec: 0.6240 | Rec: 0.7933 | F1: 0.6985\n   Val Loss: 0.2608 | Prec: 0.6426 | Rec: 0.7964 | F1: 0.7113\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.93it/s]\nEpoch 13/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 13/50:\n   Train Loss: 0.2692 | Prec: 0.6214 | Rec: 0.7927 | F1: 0.6967\n   Val Loss: 0.3073 | Prec: 0.7287 | Rec: 0.6279 | F1: 0.6745\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.99it/s]\nEpoch 14/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 14/50:\n   Train Loss: 0.2515 | Prec: 0.6435 | Rec: 0.7957 | F1: 0.7116\n   Val Loss: 0.2664 | Prec: 0.6617 | Rec: 0.7511 | F1: 0.7036\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:06<00:00,  2.01it/s]\nEpoch 15/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 15/50:\n   Train Loss: 0.2726 | Prec: 0.6194 | Rec: 0.7521 | F1: 0.6793\n   Val Loss: 0.3136 | Prec: 0.7124 | Rec: 0.6223 | F1: 0.6643\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.87it/s]\nEpoch 16/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 16/50:\n   Train Loss: 0.2418 | Prec: 0.6412 | Rec: 0.8153 | F1: 0.7178\n   Val Loss: 0.2551 | Prec: 0.6706 | Rec: 0.7730 | F1: 0.7182\nðŸ’¾ New best model saved! Val F1: 0.7182\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:06<00:00,  2.02it/s]\nEpoch 17/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 17/50:\n   Train Loss: 0.2647 | Prec: 0.6197 | Rec: 0.8069 | F1: 0.7010\n   Val Loss: 0.2690 | Prec: 0.6800 | Rec: 0.7278 | F1: 0.7031\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:06<00:00,  2.03it/s]\nEpoch 18/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 18/50:\n   Train Loss: 0.2468 | Prec: 0.6699 | Rec: 0.7898 | F1: 0.7249\n   Val Loss: 0.2628 | Prec: 0.6849 | Rec: 0.7331 | F1: 0.7082\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.99it/s]\nEpoch 19/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 19/50:\n   Train Loss: 0.2315 | Prec: 0.6795 | Rec: 0.8039 | F1: 0.7365\n   Val Loss: 0.2550 | Prec: 0.6836 | Rec: 0.7488 | F1: 0.7147\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.93it/s]\nEpoch 20/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 20/50:\n   Train Loss: 0.2237 | Prec: 0.6878 | Rec: 0.8188 | F1: 0.7476\n   Val Loss: 0.2572 | Prec: 0.6614 | Rec: 0.7660 | F1: 0.7099\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.99it/s]\nEpoch 21/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 21/50:\n   Train Loss: 0.2298 | Prec: 0.6739 | Rec: 0.8198 | F1: 0.7397\n   Val Loss: 0.2581 | Prec: 0.6914 | Rec: 0.7374 | F1: 0.7137\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.95it/s]\nEpoch 22/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 22/50:\n   Train Loss: 0.2097 | Prec: 0.6842 | Rec: 0.8300 | F1: 0.7501\n   Val Loss: 0.2610 | Prec: 0.7078 | Rec: 0.7220 | F1: 0.7148\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  2.00it/s]\nEpoch 23/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 23/50:\n   Train Loss: 0.2145 | Prec: 0.7244 | Rec: 0.8019 | F1: 0.7612\n   Val Loss: 0.2501 | Prec: 0.7013 | Rec: 0.7548 | F1: 0.7270\nðŸ’¾ New best model saved! Val F1: 0.7270\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.95it/s]\nEpoch 24/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 24/50:\n   Train Loss: 0.2122 | Prec: 0.6785 | Rec: 0.8353 | F1: 0.7488\n   Val Loss: 0.2517 | Prec: 0.6391 | Rec: 0.8289 | F1: 0.7217\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.99it/s]\nEpoch 25/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 25/50:\n   Train Loss: 0.2242 | Prec: 0.6824 | Rec: 0.8172 | F1: 0.7437\n   Val Loss: 0.2593 | Prec: 0.7000 | Rec: 0.7306 | F1: 0.7150\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.92it/s]\nEpoch 26/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 26/50:\n   Train Loss: 0.2044 | Prec: 0.7083 | Rec: 0.8376 | F1: 0.7675\n   Val Loss: 0.2562 | Prec: 0.7062 | Rec: 0.7337 | F1: 0.7197\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:06<00:00,  2.04it/s]\nEpoch 27/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 27/50:\n   Train Loss: 0.2135 | Prec: 0.6835 | Rec: 0.8412 | F1: 0.7542\n   Val Loss: 0.2663 | Prec: 0.7066 | Rec: 0.7135 | F1: 0.7101\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.98it/s]\nEpoch 28/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 28/50:\n   Train Loss: 0.2186 | Prec: 0.6994 | Rec: 0.8216 | F1: 0.7556\n   Val Loss: 0.2541 | Prec: 0.6801 | Rec: 0.7619 | F1: 0.7187\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.90it/s]\nEpoch 29/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 29/50:\n   Train Loss: 0.1919 | Prec: 0.7247 | Rec: 0.8489 | F1: 0.7819\n   Val Loss: 0.2399 | Prec: 0.6909 | Rec: 0.7968 | F1: 0.7401\nðŸ’¾ New best model saved! Val F1: 0.7401\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  2.00it/s]\nEpoch 30/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 30/50:\n   Train Loss: 0.2004 | Prec: 0.6935 | Rec: 0.8526 | F1: 0.7649\n   Val Loss: 0.2535 | Prec: 0.6959 | Rec: 0.7483 | F1: 0.7212\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:06<00:00,  2.01it/s]\nEpoch 31/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 31/50:\n   Train Loss: 0.2045 | Prec: 0.7215 | Rec: 0.8302 | F1: 0.7720\n   Val Loss: 0.2612 | Prec: 0.6996 | Rec: 0.7304 | F1: 0.7147\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.97it/s]\nEpoch 32/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 32/50:\n   Train Loss: 0.2128 | Prec: 0.6879 | Rec: 0.8324 | F1: 0.7533\n   Val Loss: 0.2534 | Prec: 0.6907 | Rec: 0.7610 | F1: 0.7241\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:06<00:00,  2.04it/s]\nEpoch 33/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 33/50:\n   Train Loss: 0.2052 | Prec: 0.7079 | Rec: 0.8368 | F1: 0.7670\n   Val Loss: 0.2489 | Prec: 0.7106 | Rec: 0.7493 | F1: 0.7294\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:06<00:00,  2.03it/s]\nEpoch 34/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 34/50:\n   Train Loss: 0.2024 | Prec: 0.7255 | Rec: 0.8335 | F1: 0.7757\n   Val Loss: 0.2535 | Prec: 0.7082 | Rec: 0.7375 | F1: 0.7225\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:06<00:00,  2.00it/s]\nEpoch 35/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 35/50:\n   Train Loss: 0.1928 | Prec: 0.7186 | Rec: 0.8507 | F1: 0.7791\n   Val Loss: 0.2352 | Prec: 0.6982 | Rec: 0.8003 | F1: 0.7458\nðŸ’¾ New best model saved! Val F1: 0.7458\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.99it/s]\nEpoch 36/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 36/50:\n   Train Loss: 0.1999 | Prec: 0.6961 | Rec: 0.8581 | F1: 0.7686\n   Val Loss: 0.2437 | Prec: 0.6879 | Rec: 0.7852 | F1: 0.7333\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.91it/s]\nEpoch 37/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 37/50:\n   Train Loss: 0.1932 | Prec: 0.6915 | Rec: 0.8666 | F1: 0.7692\n   Val Loss: 0.2415 | Prec: 0.7033 | Rec: 0.7868 | F1: 0.7427\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:06<00:00,  2.02it/s]\nEpoch 38/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 38/50:\n   Train Loss: 0.1945 | Prec: 0.7238 | Rec: 0.8353 | F1: 0.7756\n   Val Loss: 0.2476 | Prec: 0.7071 | Rec: 0.7558 | F1: 0.7306\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:06<00:00,  2.01it/s]\nEpoch 39/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 39/50:\n   Train Loss: 0.1930 | Prec: 0.7223 | Rec: 0.8456 | F1: 0.7791\n   Val Loss: 0.2558 | Prec: 0.7081 | Rec: 0.7357 | F1: 0.7216\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:06<00:00,  2.03it/s]\nEpoch 40/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 40/50:\n   Train Loss: 0.1967 | Prec: 0.7105 | Rec: 0.8400 | F1: 0.7698\n   Val Loss: 0.2393 | Prec: 0.7133 | Rec: 0.7761 | F1: 0.7434\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.94it/s]\nEpoch 41/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 41/50:\n   Train Loss: 0.2028 | Prec: 0.7179 | Rec: 0.8435 | F1: 0.7757\n   Val Loss: 0.2375 | Prec: 0.7009 | Rec: 0.7800 | F1: 0.7383\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:06<00:00,  2.00it/s]\nEpoch 42/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 42/50:\n   Train Loss: 0.1918 | Prec: 0.7074 | Rec: 0.8601 | F1: 0.7763\n   Val Loss: 0.2499 | Prec: 0.7035 | Rec: 0.7445 | F1: 0.7234\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.87it/s]\nEpoch 43/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 43/50:\n   Train Loss: 0.1934 | Prec: 0.7158 | Rec: 0.8448 | F1: 0.7749\n   Val Loss: 0.2449 | Prec: 0.6805 | Rec: 0.7817 | F1: 0.7276\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.93it/s]\nEpoch 44/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 44/50:\n   Train Loss: 0.2149 | Prec: 0.7172 | Rec: 0.8420 | F1: 0.7746\n   Val Loss: 0.2497 | Prec: 0.6866 | Rec: 0.7676 | F1: 0.7249\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.92it/s]\nEpoch 45/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 45/50:\n   Train Loss: 0.1995 | Prec: 0.7092 | Rec: 0.8424 | F1: 0.7701\n   Val Loss: 0.2436 | Prec: 0.6897 | Rec: 0.7786 | F1: 0.7314\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.92it/s]\nEpoch 46/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 46/50:\n   Train Loss: 0.1884 | Prec: 0.7170 | Rec: 0.8616 | F1: 0.7827\n   Val Loss: 0.2373 | Prec: 0.6938 | Rec: 0.7932 | F1: 0.7401\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.96it/s]\nEpoch 47/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 47/50:\n   Train Loss: 0.2056 | Prec: 0.7039 | Rec: 0.8449 | F1: 0.7680\n   Val Loss: 0.2330 | Prec: 0.6872 | Rec: 0.8100 | F1: 0.7435\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:06<00:00,  2.03it/s]\nEpoch 48/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 48/50:\n   Train Loss: 0.2126 | Prec: 0.7036 | Rec: 0.8362 | F1: 0.7642\n   Val Loss: 0.2378 | Prec: 0.6986 | Rec: 0.7859 | F1: 0.7397\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.94it/s]\nEpoch 49/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 49/50:\n   Train Loss: 0.2053 | Prec: 0.7157 | Rec: 0.8316 | F1: 0.7693\n   Val Loss: 0.2391 | Prec: 0.6957 | Rec: 0.7867 | F1: 0.7384\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.96it/s]\nEpoch 50/50 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ“ˆ Epoch 50/50:\n   Train Loss: 0.1815 | Prec: 0.7244 | Rec: 0.8650 | F1: 0.7885\n   Val Loss: 0.2356 | Prec: 0.6936 | Rec: 0.7974 | F1: 0.7419\n\nðŸ§ª Testing VM-UNet on test set...\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  4.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ” Finding optimal threshold...\n   Threshold 0.3: Precision=0.7251, Recall=0.7229, F1=0.7240\n   Threshold 0.4: Precision=0.7513, Recall=0.6867, F1=0.7175\n   Threshold 0.5: Precision=0.7751, Recall=0.6499, F1=0.7070\n   Threshold 0.6: Precision=0.7977, Recall=0.6106, F1=0.6917\n   Threshold 0.7: Precision=0.8210, Recall=0.5648, F1=0.6692\n\n============================================================\nðŸŽ¯ VM-UNet FINAL TEST RESULTS\n============================================================\nOptimal Threshold: 0.3\nTest Loss: 0.2495\nPrecision: 0.7251\nRecall:    0.7229\nAccuracy:  0.9037\nF1-Score:  0.7240\n============================================================\n\nðŸ’¾ Final model saved!\n","output_type":"stream"}],"execution_count":8}]}