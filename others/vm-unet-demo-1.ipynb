{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13467467,"sourceType":"datasetVersion","datasetId":8549022}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms.functional as TF\nimport random\n\n# Data path\ndata_path = r\"/kaggle/input/monusegdataset445/data\"\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-22T15:55:46.600261Z","iopub.execute_input":"2025-10-22T15:55:46.600895Z","iopub.status.idle":"2025-10-22T15:55:46.605853Z","shell.execute_reply.started":"2025-10-22T15:55:46.600871Z","shell.execute_reply":"2025-10-22T15:55:46.605069Z"}},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":"Imports all required libraries (PyTorch, torchvision, NumPy, PIL) and defines dataset path.","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# DATASET PROCESSING\n# ============================================================================\n\n# Data Augmentation Class\nclass NucleiAugmentation:\n    def __init__(self, image_size=256, augmentation_prob=0.7, sparse_mode=False):\n        self.image_size = image_size\n        self.augmentation_prob = augmentation_prob\n        self.sparse_mode = sparse_mode\n    \n    def __call__(self, image, mask):\n        if torch.is_tensor(image):\n            image = TF.to_pil_image(image)\n        if torch.is_tensor(mask):\n            mask = TF.to_pil_image(mask)\n        \n        # Random horizontal flip\n        if random.random() > self.augmentation_prob:\n            image = TF.hflip(image)\n            mask = TF.hflip(mask)\n        \n        # Random vertical flip\n        if random.random() > self.augmentation_prob:\n            image = TF.vflip(image)\n            mask = TF.vflip(mask)\n        \n        # Random rotation (-45 to +45 degrees)\n        if random.random() > self.augmentation_prob:\n            angle = random.uniform(-45, 45)\n            image = TF.rotate(image, angle)\n            mask = TF.rotate(mask, angle)\n        \n        # Random brightness/contrast adjustment\n        if random.random() > self.augmentation_prob:\n            brightness_factor = random.uniform(0.8, 1.2)\n            image = TF.adjust_brightness(image, brightness_factor)\n        \n        if random.random() > self.augmentation_prob:\n            contrast_factor = random.uniform(0.8, 1.2)\n            image = TF.adjust_contrast(image, contrast_factor)\n        \n        # Random Gaussian blur\n        if random.random() > self.augmentation_prob:\n            image = TF.gaussian_blur(image, kernel_size=3)\n        \n        # For sparse mode: random crop\n        if self.sparse_mode and random.random() > 0.8:\n            width, height = image.size\n            crop_size = random.randint(128, 256)\n            i = random.randint(0, height - crop_size)\n            j = random.randint(0, width - crop_size)\n            image = TF.crop(image, i, j, crop_size, crop_size)\n            mask = TF.crop(mask, i, j, crop_size, crop_size)\n        \n        # Ensure final size\n        image = image.resize((self.image_size, self.image_size))\n        mask = mask.resize((self.image_size, self.image_size))\n        \n        return image, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T15:55:46.607173Z","iopub.execute_input":"2025-10-22T15:55:46.607386Z","iopub.status.idle":"2025-10-22T15:55:46.622026Z","shell.execute_reply.started":"2025-10-22T15:55:46.607370Z","shell.execute_reply":"2025-10-22T15:55:46.621253Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"class KMMSDataset(Dataset):\n    def __init__(self, image_paths, mask_paths, image_size=256, augment=False, sparse_mode=False):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.image_size = image_size\n        self.augment = augment\n        self.sparse_mode = sparse_mode\n        self.augmentor = NucleiAugmentation(image_size, sparse_mode=sparse_mode) if augment else None\n        \n        self.valid_pairs = list(zip(image_paths, mask_paths))\n        \n        print(f\"Created dataset with {len(self.valid_pairs)} valid image-mask pairs\")\n        if augment:\n            mode = \"sparse\" if sparse_mode else \"normal\"\n            print(f\"✓ Data augmentation ENABLED ({mode} mode)\")\n        else:\n            print(\"✗ Data augmentation DISABLED\")\n    \n    def __len__(self):\n        return len(self.valid_pairs)\n    \n    def __getitem__(self, idx):\n        img_path, mask_path = self.valid_pairs[idx]\n        \n        try:\n            # Load image\n            image = Image.open(img_path).convert('RGB')\n            mask = Image.open(mask_path).convert('L')\n            \n            # Apply augmentation\n            if self.augment and self.augmentor:\n                image, mask = self.augmentor(image, mask)\n            else:\n                image = image.resize((self.image_size, self.image_size))\n                mask = mask.resize((self.image_size, self.image_size))\n            \n            # Convert to numpy arrays\n            image = np.array(image).astype(np.float32) / 255.0\n            mask = np.array(mask).astype(np.float32) / 255.0\n            \n            # Ensure mask is binary\n            mask = (mask > 0.5).astype(np.float32)\n            \n            # Convert to tensors\n            image = torch.from_numpy(image).permute(2, 0, 1).float()\n            mask = torch.from_numpy(mask).unsqueeze(0).float()\n            \n            return image, mask, os.path.basename(img_path)\n        \n        except Exception as e:\n            print(f\"Error loading {img_path}: {e}\")\n            dummy_image = torch.zeros(3, self.image_size, self.image_size)\n            dummy_mask = torch.zeros(1, self.image_size, self.image_size)\n            return dummy_image, dummy_mask, \"error\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T15:55:46.623047Z","iopub.execute_input":"2025-10-22T15:55:46.623309Z","iopub.status.idle":"2025-10-22T15:55:46.641075Z","shell.execute_reply.started":"2025-10-22T15:55:46.623290Z","shell.execute_reply":"2025-10-22T15:55:46.640320Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# Data loading and balancing\ndef find_kmms_data():\n    train_path = os.path.join(data_path, \"kmms_training\")\n    test_path = os.path.join(data_path, \"kmms_test\")\n    \n    train_images, train_masks, test_images, test_masks = [], [], [], []\n    \n    # Load training data\n    if os.path.exists(train_path):\n        train_images_dir = os.path.join(train_path, \"images\")\n        train_masks_dir = os.path.join(train_path, \"masks\")\n        if os.path.exists(train_images_dir):\n            train_images = sorted([os.path.join(train_images_dir, f) for f in os.listdir(train_images_dir) \n                                 if f.lower().endswith(('.tiff', '.tif', '.png', '.jpg', '.jpeg'))])\n        if os.path.exists(train_masks_dir):\n            train_masks = sorted([os.path.join(train_masks_dir, f) for f in os.listdir(train_masks_dir) \n                                if f.lower().endswith(('.tiff', '.tif', '.png', '.jpg', '.jpeg'))])\n    \n    # Load test data\n    if os.path.exists(test_path):\n        test_images_dir = os.path.join(test_path, \"images\")\n        test_masks_dir = os.path.join(test_path, \"masks\")\n        if os.path.exists(test_images_dir):\n            test_images = sorted([os.path.join(test_images_dir, f) for f in os.listdir(test_images_dir) \n                                if f.lower().endswith(('.tiff', '.tif', '.png', '.jpg', '.jpeg'))])\n        if os.path.exists(test_masks_dir):\n            test_masks = sorted([os.path.join(test_masks_dir, f) for f in os.listdir(test_masks_dir) \n                               if f.lower().endswith(('.tiff', '.tif', '.png', '.jpg', '.jpeg'))])\n    \n    print(f\"Found: {len(train_images)} training images, {len(test_images)} test images\")\n    return train_images, train_masks, test_images, test_masks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T15:55:46.641970Z","iopub.execute_input":"2025-10-22T15:55:46.642354Z","iopub.status.idle":"2025-10-22T15:55:46.657984Z","shell.execute_reply.started":"2025-10-22T15:55:46.642325Z","shell.execute_reply":"2025-10-22T15:55:46.657250Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# Improved data splitting function\ndef create_balanced_split(all_images, all_masks, test_size=0.15, val_size=0.15, random_state=42):\n    \"\"\"\n    Create a balanced train/val/test split from all available data\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n    \n    # First split: Train+Val vs Test\n    train_val_images, test_images, train_val_masks, test_masks = train_test_split(\n        all_images, all_masks, test_size=test_size, random_state=random_state\n    )\n    \n    # Second split: Train vs Val from train_val set\n    val_ratio = val_size / (1 - test_size)\n    train_images, val_images, train_masks, val_masks = train_test_split(\n        train_val_images, train_val_masks, test_size=val_ratio, random_state=random_state\n    )\n    \n    total_samples = len(all_images)\n    print(f\"✅ Balanced Data Split:\")\n    print(f\"   Training: {len(train_images)} images ({len(train_images)/total_samples*100:.1f}%)\")\n    print(f\"   Validation: {len(val_images)} images ({len(val_images)/total_samples*100:.1f}%)\")\n    print(f\"   Test: {len(test_images)} images ({len(test_images)/total_samples*100:.1f}%)\")\n    \n    return train_images, val_images, test_images, train_masks, val_masks, test_masks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T15:55:46.659555Z","iopub.execute_input":"2025-10-22T15:55:46.659797Z","iopub.status.idle":"2025-10-22T15:55:46.673388Z","shell.execute_reply.started":"2025-10-22T15:55:46.659747Z","shell.execute_reply":"2025-10-22T15:55:46.672502Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# Main dataset preparation function\ndef prepare_datasets(image_size=256, batch_size=4, balanced_split=True):\n    \"\"\"\n    Main function to prepare datasets with optional balanced splitting\n    \"\"\"\n    # Load all data\n    train_images, train_masks, test_images, test_masks = find_kmms_data()\n    \n    if balanced_split:\n        # Combine all data for balanced splitting\n        all_images = train_images + test_images\n        all_masks = train_masks + test_masks\n        \n        # Create balanced split (70-15-15 recommended)\n        train_images, val_images, test_images, train_masks, val_masks, test_masks = create_balanced_split(\n            all_images, all_masks, test_size=0.15, val_size=0.15\n        )\n    else:\n        # Use original split (80-20 from training data for validation)\n        from sklearn.model_selection import train_test_split\n        train_images, val_images, train_masks, val_masks = train_test_split(\n            train_images, train_masks, test_size=0.2, random_state=42\n        )\n        print(f\"Using original split: Train={len(train_images)}, Val={len(val_images)}, Test={len(test_images)}\")\n    \n    # Create datasets\n    train_dataset = KMMSDataset(train_images, train_masks, image_size=image_size, augment=True, sparse_mode=True)\n    val_dataset = KMMSDataset(val_images, val_masks, image_size=image_size, augment=False, sparse_mode=False)\n    test_dataset = KMMSDataset(test_images, test_masks, image_size=image_size, augment=False, sparse_mode=False)\n    \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    \n    return train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T15:55:46.764345Z","iopub.execute_input":"2025-10-22T15:55:46.764802Z","iopub.status.idle":"2025-10-22T15:55:46.772829Z","shell.execute_reply.started":"2025-10-22T15:55:46.764744Z","shell.execute_reply":"2025-10-22T15:55:46.771995Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# Example usage\nif __name__ == \"__main__\":\n    # Prepare datasets with balanced splitting (recommended)\n    train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = prepare_datasets(\n        image_size=256, \n        batch_size=4, \n        balanced_split=True  # Set to False to use original split\n    )\n    \n    print(f\"\\nFinal dataset sizes:\")\n    print(f\"Training: {len(train_dataset)} samples\")\n    print(f\"Validation: {len(val_dataset)} samples\") \n    print(f\"Test: {len(test_dataset)} samples\")\n    \n    # Test data loading\n    print(f\"\\nTesting data loader...\")\n    for images, masks, filenames in train_loader:\n        print(f\"Batch - Images: {images.shape}, Masks: {masks.shape}\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T15:55:46.773922Z","iopub.execute_input":"2025-10-22T15:55:46.774111Z","iopub.status.idle":"2025-10-22T15:55:47.157572Z","shell.execute_reply.started":"2025-10-22T15:55:46.774096Z","shell.execute_reply":"2025-10-22T15:55:47.156449Z"}},"outputs":[{"name":"stdout","text":"Found: 24 training images, 58 test images\n✅ Balanced Data Split:\n   Training: 56 images (68.3%)\n   Validation: 13 images (15.9%)\n   Test: 13 images (15.9%)\nCreated dataset with 56 valid image-mask pairs\n✓ Data augmentation ENABLED (sparse mode)\nCreated dataset with 13 valid image-mask pairs\n✗ Data augmentation DISABLED\nCreated dataset with 13 valid image-mask pairs\n✗ Data augmentation DISABLED\n\nFinal dataset sizes:\nTraining: 56 samples\nValidation: 13 samples\nTest: 13 samples\n\nTesting data loader...\nBatch - Images: torch.Size([4, 3, 256, 256]), Masks: torch.Size([4, 1, 256, 256])\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ============================================================================\n# FIXED VM-UNET MODEL ARCHITECTURE\n# ============================================================================\n\nclass VMBlock(nn.Module):\n    \"\"\"Vision Mamba Block with residual connection\"\"\"\n    def __init__(self, channels):\n        super().__init__()\n        self.norm = nn.LayerNorm(channels)\n        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.activation = nn.GELU()\n        \n    def forward(self, x):\n        residual = x\n        # LayerNorm and channel-first for conv\n        x = x.permute(0, 2, 3, 1)  # [B, C, H, W] -> [B, H, W, C]\n        x = self.norm(x)\n        x = x.permute(0, 3, 1, 2)  # [B, H, W, C] -> [B, C, H, W]\n        \n        x = self.conv1(x)\n        x = self.activation(x)\n        x = self.conv2(x)\n        \n        return x + residual\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.GELU(),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.GELU()\n        )\n    \n    def forward(self, x):\n        return self.double_conv(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T15:55:47.158785Z","iopub.execute_input":"2025-10-22T15:55:47.159045Z","iopub.status.idle":"2025-10-22T15:55:47.168048Z","shell.execute_reply.started":"2025-10-22T15:55:47.159020Z","shell.execute_reply":"2025-10-22T15:55:47.167183Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"class VMEncoderBlock(nn.Module):\n    \"\"\"Encoder block with VMBlock\"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = DoubleConv(in_channels, out_channels)\n        self.vm_block = VMBlock(out_channels)\n        self.pool = nn.MaxPool2d(2)\n    \n    def forward(self, x):\n        # Feature extraction\n        x = self.conv(x)\n        # Mamba-style processing\n        x = self.vm_block(x)\n        # Downsample\n        pooled = self.pool(x)\n        return pooled, x\n\nclass VMDecoderBlock(nn.Module):\n    \"\"\"FIXED Decoder block with proper channel handling\"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        # Upsample reduces channels by 2\n        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n        # After concatenation: (in_channels//2 + skip_channels) -> out_channels\n        self.conv = DoubleConv(in_channels // 2 + out_channels, out_channels)  # FIXED\n        self.vm_block = VMBlock(out_channels)\n    \n    def forward(self, x, skip):\n        # Upsample\n        x = self.up(x)\n        # Skip connection - ensure spatial dimensions match\n        if x.shape[2:] != skip.shape[2:]:\n            x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=True)\n        \n        # Concatenate along channel dimension\n        x = torch.cat([x, skip], dim=1)\n        # Feature extraction\n        x = self.conv(x)\n        # Mamba-style processing\n        x = self.vm_block(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T15:55:47.169981Z","iopub.execute_input":"2025-10-22T15:55:47.170180Z","iopub.status.idle":"2025-10-22T15:55:47.186512Z","shell.execute_reply.started":"2025-10-22T15:55:47.170165Z","shell.execute_reply":"2025-10-22T15:55:47.185792Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"class VMUNet(nn.Module):\n    def __init__(self, n_channels=3, n_classes=1):\n        super().__init__()\n        \n        # Encoder path\n        self.enc1 = VMEncoderBlock(n_channels, 64)    # 64 channels\n        self.enc2 = VMEncoderBlock(64, 128)           # 128 channels  \n        self.enc3 = VMEncoderBlock(128, 256)          # 256 channels\n        self.enc4 = VMEncoderBlock(256, 512)          # 512 channels\n        \n        # Bridge\n        self.bridge = DoubleConv(512, 1024)           # 1024 channels\n        \n        # Decoder path - FIXED channel dimensions\n        self.dec1 = VMDecoderBlock(1024, 512)         # 1024->512 after up, then 512+512=1024->512\n        self.dec2 = VMDecoderBlock(512, 256)          # 512->256 after up, then 256+256=512->256\n        self.dec3 = VMDecoderBlock(256, 128)          # 256->128 after up, then 128+128=256->128\n        self.dec4 = VMDecoderBlock(128, 64)           # 128->64 after up, then 64+64=128->64\n        \n        # Output\n        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n        \n    def forward(self, x):\n        # Encoder\n        x1, skip1 = self.enc1(x)    # skip1: 64 channels\n        x2, skip2 = self.enc2(x1)   # skip2: 128 channels\n        x3, skip3 = self.enc3(x2)   # skip3: 256 channels\n        x4, skip4 = self.enc4(x3)   # skip4: 512 channels\n        \n        # Bridge\n        x5 = self.bridge(x4)        # 1024 channels\n        \n        # Decoder with proper skip connections\n        x = self.dec1(x5, skip4)    # 512 channels\n        x = self.dec2(x, skip3)     # 256 channels\n        x = self.dec3(x, skip2)     # 128 channels\n        x = self.dec4(x, skip1)     # 64 channels\n        \n        # Output\n        return torch.sigmoid(self.outc(x))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T15:55:47.187273Z","iopub.execute_input":"2025-10-22T15:55:47.187578Z","iopub.status.idle":"2025-10-22T15:55:47.204845Z","shell.execute_reply.started":"2025-10-22T15:55:47.187560Z","shell.execute_reply":"2025-10-22T15:55:47.204136Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"# ============================================================================\n# SIMPLIFIED ENHANCED VERSION (Easier to debug)\n# ============================================================================\n\nclass chr(nn.Module):\n    \"\"\"Simplified version that's easier to debug\"\"\"\n    def __init__(self, n_channels=3, n_classes=1):\n        super().__init__()\n        \n        # Encoder\n        self.enc1 = DoubleConv(n_channels, 64)\n        self.pool1 = nn.MaxPool2d(2)\n        self.enc2 = DoubleConv(64, 128)\n        self.pool2 = nn.MaxPool2d(2)\n        self.enc3 = DoubleConv(128, 256)\n        self.pool3 = nn.MaxPool2d(2)\n        self.enc4 = DoubleConv(256, 512)\n        self.pool4 = nn.MaxPool2d(2)\n        \n        # Bridge with VMBlock\n        self.bridge = DoubleConv(512, 1024)\n        self.vm_bridge = VMBlock(1024)\n        \n        # Decoder\n        self.up1 = nn.ConvTranspose2d(1024, 512, 2, 2)\n        self.dec1 = DoubleConv(1024, 512)  # 512 (up) + 512 (skip)\n        self.vm1 = VMBlock(512)\n        \n        self.up2 = nn.ConvTranspose2d(512, 256, 2, 2)\n        self.dec2 = DoubleConv(512, 256)   # 256 (up) + 256 (skip)\n        self.vm2 = VMBlock(256)\n        \n        self.up3 = nn.ConvTranspose2d(256, 128, 2, 2)\n        self.dec3 = DoubleConv(256, 128)   # 128 (up) + 128 (skip)\n        self.vm3 = VMBlock(128)\n        \n        self.up4 = nn.ConvTranspose2d(128, 64, 2, 2)\n        self.dec4 = DoubleConv(128, 64)    # 64 (up) + 64 (skip)\n        self.vm4 = VMBlock(64)\n        \n        self.outc = nn.Conv2d(64, n_classes, 1)\n        \n    def forward(self, x):\n        # Encoder\n        e1 = self.enc1(x)\n        e2 = self.enc2(self.pool1(e1))\n        e3 = self.enc3(self.pool2(e2))\n        e4 = self.enc4(self.pool3(e3))\n        \n        # Bridge\n        b = self.pool4(e4)\n        b = self.bridge(b)\n        b = self.vm_bridge(b)\n        \n        # Decoder\n        d1 = self.up1(b)\n        d1 = torch.cat([d1, e4], dim=1)\n        d1 = self.dec1(d1)\n        d1 = self.vm1(d1)\n        \n        d2 = self.up2(d1)\n        d2 = torch.cat([d2, e3], dim=1)\n        d2 = self.dec2(d2)\n        d2 = self.vm2(d2)\n        \n        d3 = self.up3(d2)\n        d3 = torch.cat([d3, e2], dim=1)\n        d3 = self.dec3(d3)\n        d3 = self.vm3(d3)\n        \n        d4 = self.up4(d3)\n        d4 = torch.cat([d4, e1], dim=1)\n        d4 = self.dec4(d4)\n        d4 = self.vm4(d4)\n        \n        return torch.sigmoid(self.outc(d4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T15:55:47.205727Z","iopub.execute_input":"2025-10-22T15:55:47.206030Z","iopub.status.idle":"2025-10-22T15:55:47.222392Z","shell.execute_reply.started":"2025-10-22T15:55:47.206012Z","shell.execute_reply":"2025-10-22T15:55:47.221654Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ============================================================================\n# LOSS FUNCTIONS (ADD THIS SECTION)\n# ============================================================================\n\nclass DiceLoss(nn.Module):\n    def __init__(self, smooth=1e-6):\n        super().__init__()\n        self.smooth = smooth\n    \n    def forward(self, inputs, targets):\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()\n        dice = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)\n        return 1 - dice\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.8, gamma=2.0):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n    \n    def forward(self, inputs, targets):\n        bce_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-bce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n        return focal_loss.mean()\n\nclass CombinedLoss(nn.Module):\n    def __init__(self, alpha=0.7, beta=0.3):\n        super().__init__()\n        self.dice_loss = DiceLoss()\n        self.focal_loss = FocalLoss()\n        \n    def forward(self, inputs, targets):\n        dice = self.dice_loss(inputs, targets)\n        focal = self.focal_loss(inputs, targets)\n        return 0.6 * dice + 0.4 * focal\n\n# ============================================================================\n# METRICS FUNCTION (ADD THIS TOO)\n# ============================================================================\n\ndef calculate_metrics(predictions, targets, threshold=0.5):\n    \"\"\"Calculate precision, recall, accuracy, and F1-score\"\"\"\n    pred_binary = (predictions > threshold).float()\n    target_binary = (targets > threshold).float()\n    \n    pred_flat = pred_binary.view(-1).cpu().numpy()\n    target_flat = target_binary.view(-1).cpu().numpy()\n    \n    from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n    \n    precision = precision_score(target_flat, pred_flat, zero_division=0)\n    recall = recall_score(target_flat, pred_flat, zero_division=0)\n    accuracy = accuracy_score(target_flat, pred_flat)\n    f1 = f1_score(target_flat, pred_flat, zero_division=0)\n    \n    return precision, recall, accuracy, f1\n\n# ============================================================================\n# TEST FUNCTION (ADD THIS)\n# ============================================================================\n\ndef test_vm_unet(model, test_loader, device='cuda'):\n    \"\"\"Test the trained model\"\"\"\n    model.eval()\n    model.to(device)\n    \n    test_preds = []\n    test_targets = []\n    test_loss = 0.0\n    criterion = CombinedLoss(alpha=0.7, beta=0.3)\n    \n    print(\"\\n🧪 Testing VM-UNet on test set...\")\n    \n    with torch.no_grad():\n        for images, masks, _ in tqdm(test_loader, desc='Testing'):\n            images = images.to(device)\n            masks = masks.to(device)\n            \n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            test_loss += loss.item()\n            \n            test_preds.append(outputs)\n            test_targets.append(masks)\n    \n    test_preds = torch.cat(test_preds)\n    test_targets = torch.cat(test_targets)\n    \n    # Calculate metrics with different thresholds\n    thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n    best_f1 = 0\n    best_threshold = 0.5\n    \n    print(\"\\n🔍 Finding optimal threshold...\")\n    for threshold in thresholds:\n        precision, recall, accuracy, f1 = calculate_metrics(test_preds, test_targets, threshold=threshold)\n        print(f'   Threshold {threshold}: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}')\n        \n        if f1 > best_f1:\n            best_f1 = f1\n            best_threshold = threshold\n    \n    avg_test_loss = test_loss / len(test_loader)\n    \n    # Final results\n    print(\"\\n\" + \"=\"*60)\n    print(\"🎯 VM-UNet FINAL TEST RESULTS\")\n    print(\"=\"*60)\n    print(f\"Optimal Threshold: {best_threshold}\")\n    print(f\"Test Loss: {avg_test_loss:.4f}\")\n    \n    precision, recall, accuracy, f1 = calculate_metrics(test_preds, test_targets, threshold=best_threshold)\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall:    {recall:.4f}\")\n    print(f\"Accuracy:  {accuracy:.4f}\")\n    print(f\"F1-Score:  {f1:.4f}\")\n    print(\"=\"*60)\n    \n    return precision, recall, accuracy, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T15:55:47.223340Z","iopub.execute_input":"2025-10-22T15:55:47.223816Z","iopub.status.idle":"2025-10-22T15:55:47.242122Z","shell.execute_reply.started":"2025-10-22T15:55:47.223793Z","shell.execute_reply":"2025-10-22T15:55:47.241440Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# ============================================================================\n# TEST THE MODEL WITH YOUR DATA\n# ============================================================================\n\ndef test_model_with_data():\n    \"\"\"Test the model with your data loader to verify it works\"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Testing on device: {device}\")\n    \n    # Load your data\n    train_loader, val_loader, test_loader, _, _, _ = prepare_datasets(\n        image_size=256, \n        batch_size=4, \n        balanced_split=True\n    )\n    \n    # Test both models\n    models = {\n        \"Fixed VM-UNet\": VMUNet(n_channels=3, n_classes=1),\n        \"Simple VM-UNet\": SimpleVMUNet(n_channels=3, n_classes=1)\n    }\n    \n    for name, model in models.items():\n        print(f\"\\n{'='*50}\")\n        print(f\"Testing {name}\")\n        print(f\"{'='*50}\")\n        \n        model.to(device)\n        \n        # Test with a batch\n        try:\n            with torch.no_grad():\n                for images, masks, _ in train_loader:\n                    images = images.to(device)\n                    print(f\"Input shape: {images.shape}\")\n                    \n                    outputs = model(images)\n                    print(f\"Output shape: {outputs.shape}\")\n                    print(f\"✅ {name} works correctly!\")\n                    break\n                    \n        except Exception as e:\n            print(f\"❌ Error in {name}: {e}\")\n    \n    return models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T15:55:47.242890Z","iopub.execute_input":"2025-10-22T15:55:47.243146Z","iopub.status.idle":"2025-10-22T15:55:47.259134Z","shell.execute_reply.started":"2025-10-22T15:55:47.243129Z","shell.execute_reply":"2025-10-22T15:55:47.258401Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":" #============================================================================\n# UPDATED TRAINING FUNCTION\n# ============================================================================\n\ndef train_fixed_vm_unet(model, train_loader, val_loader, num_epochs=50, device='cuda'):\n    model.to(device)\n    \n    criterion = CombinedLoss(alpha=0.7, beta=0.3)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n    \n    train_losses = []\n    val_losses = []\n    train_f1_scores = []\n    val_f1_scores = []\n    \n    best_val_f1 = 0.0\n    best_model_state = None\n    \n    print(\"🚀 Starting Fixed VM-UNet Training...\")\n    \n    for epoch in range(num_epochs):\n        # Training\n        model.train()\n        epoch_train_loss = 0.0\n        train_preds = []\n        train_targets = []\n        \n        for images, masks, _ in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]'):\n            images = images.to(device)\n            masks = masks.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            loss.backward()\n            optimizer.step()\n            \n            epoch_train_loss += loss.item()\n            train_preds.append(outputs.detach())\n            train_targets.append(masks.detach())\n        \n        # Validation\n        model.eval()\n        epoch_val_loss = 0.0\n        val_preds = []\n        val_targets = []\n        \n        with torch.no_grad():\n            for images, masks, _ in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]'):\n                images = images.to(device)\n                masks = masks.to(device)\n                \n                outputs = model(images)\n                loss = criterion(outputs, masks)\n                epoch_val_loss += loss.item()\n                \n                val_preds.append(outputs)\n                val_targets.append(masks)\n        \n        # Calculate metrics\n        avg_train_loss = epoch_train_loss / len(train_loader)\n        avg_val_loss = epoch_val_loss / len(val_loader)\n        \n        train_preds = torch.cat(train_preds)\n        train_targets = torch.cat(train_targets)\n        train_precision, train_recall, train_accuracy, train_f1 = calculate_metrics(train_preds, train_targets, threshold=0.3)\n        \n        val_preds = torch.cat(val_preds)\n        val_targets = torch.cat(val_targets)\n        val_precision, val_recall, val_accuracy, val_f1 = calculate_metrics(val_preds, val_targets, threshold=0.3)\n        \n        # Store results\n        train_losses.append(avg_train_loss)\n        val_losses.append(avg_val_loss)\n        train_f1_scores.append(train_f1)\n        val_f1_scores.append(val_f1)\n        \n        scheduler.step(avg_val_loss)\n        \n        print(f'\\n📈 Epoch {epoch+1}/{num_epochs}:')\n        print(f'   Train Loss: {avg_train_loss:.4f} | Prec: {train_precision:.4f} | Rec: {train_recall:.4f} | F1: {train_f1:.4f}')\n        print(f'   Val Loss: {avg_val_loss:.4f} | Prec: {val_precision:.4f} | Rec: {val_recall:.4f} | F1: {val_f1:.4f}')\n        \n        # Save best model\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            best_model_state = model.state_dict().copy()\n            torch.save(best_model_state, 'best_fixed_vm_unet_model.pth')\n            print(f'💾 New best model saved! Val F1: {best_val_f1:.4f}')\n    \n    if best_model_state is not None:\n        model.load_state_dict(best_model_state)\n    return model, train_losses, val_losses, train_f1_scores, val_f1_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T15:55:47.260033Z","iopub.execute_input":"2025-10-22T15:55:47.260325Z","iopub.status.idle":"2025-10-22T15:55:47.278070Z","shell.execute_reply.started":"2025-10-22T15:55:47.260309Z","shell.execute_reply":"2025-10-22T15:55:47.277241Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\nfrom tqdm import tqdm\ndef main():\n    # First test the models\n    models = test_model_with_data()\n    \n    # Use the simple model for training (more stable)\n    model = models[\"Simple VM-UNet\"]\n    \n    # Load datasets\n    train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = prepare_datasets(\n        image_size=256, \n        batch_size=4, \n        balanced_split=True\n    )\n    \n    print(f\"\\n📊 Dataset sizes: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n    \n    # Train the model\n    print(\"\\n🎯 Starting training...\")\n    model, train_losses, val_losses, train_f1_scores, val_f1_scores = train_fixed_vm_unet(\n        model, train_loader, val_loader, num_epochs=50, device='cuda'\n    )\n    \n    # Test the model\n    test_metrics = test_vm_unet(model, test_loader, device='cuda')\n    \n    # Save final model\n    torch.save(model.state_dict(), 'final_fixed_vm_unet_model.pth')\n    print(\"\\n💾 Final model saved!\")\n    \n    return model, test_metrics\n\nif __name__ == \"__main__\":\n    model, metrics = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T15:55:47.280118Z","iopub.execute_input":"2025-10-22T15:55:47.280472Z","iopub.status.idle":"2025-10-22T16:11:37.912796Z","shell.execute_reply.started":"2025-10-22T15:55:47.280455Z","shell.execute_reply":"2025-10-22T16:11:37.911695Z"}},"outputs":[{"name":"stdout","text":"Testing on device: cuda\nFound: 24 training images, 58 test images\n✅ Balanced Data Split:\n   Training: 56 images (68.3%)\n   Validation: 13 images (15.9%)\n   Test: 13 images (15.9%)\nCreated dataset with 56 valid image-mask pairs\n✓ Data augmentation ENABLED (sparse mode)\nCreated dataset with 13 valid image-mask pairs\n✗ Data augmentation DISABLED\nCreated dataset with 13 valid image-mask pairs\n✗ Data augmentation DISABLED\n\n==================================================\nTesting Fixed VM-UNet\n==================================================\nInput shape: torch.Size([4, 3, 256, 256])\nOutput shape: torch.Size([4, 1, 256, 256])\n✅ Fixed VM-UNet works correctly!\n\n==================================================\nTesting Simple VM-UNet\n==================================================\nInput shape: torch.Size([4, 3, 256, 256])\nOutput shape: torch.Size([4, 1, 256, 256])\n✅ Simple VM-UNet works correctly!\nFound: 24 training images, 58 test images\n✅ Balanced Data Split:\n   Training: 56 images (68.3%)\n   Validation: 13 images (15.9%)\n   Test: 13 images (15.9%)\nCreated dataset with 56 valid image-mask pairs\n✓ Data augmentation ENABLED (sparse mode)\nCreated dataset with 13 valid image-mask pairs\n✗ Data augmentation DISABLED\nCreated dataset with 13 valid image-mask pairs\n✗ Data augmentation DISABLED\n\n📊 Dataset sizes: Train=56, Val=13, Test=13\n\n🎯 Starting training...\n🚀 Starting Fixed VM-UNet Training...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50 [Train]: 100%|██████████| 14/14 [00:05<00:00,  2.42it/s]\nEpoch 1/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 1/50:\n   Train Loss: 0.4825 | Prec: 0.1710 | Rec: 0.9743 | F1: 0.2909\n   Val Loss: 0.4750 | Prec: 0.4954 | Rec: 0.5029 | F1: 0.4991\n💾 New best model saved! Val F1: 0.4991\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.26it/s]\nEpoch 2/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 2/50:\n   Train Loss: 0.3601 | Prec: 0.4032 | Rec: 0.8535 | F1: 0.5477\n   Val Loss: 0.4853 | Prec: 0.6966 | Rec: 0.3516 | F1: 0.4673\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.23it/s]\nEpoch 3/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 3/50:\n   Train Loss: 0.3469 | Prec: 0.4730 | Rec: 0.7805 | F1: 0.5890\n   Val Loss: 0.3622 | Prec: 0.6635 | Rec: 0.5872 | F1: 0.6230\n💾 New best model saved! Val F1: 0.6230\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.23it/s]\nEpoch 4/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 4/50:\n   Train Loss: 0.2895 | Prec: 0.5828 | Rec: 0.7616 | F1: 0.6603\n   Val Loss: 0.2939 | Prec: 0.6498 | Rec: 0.7311 | F1: 0.6881\n💾 New best model saved! Val F1: 0.6881\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.17it/s]\nEpoch 5/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 5/50:\n   Train Loss: 0.2784 | Prec: 0.5908 | Rec: 0.8024 | F1: 0.6805\n   Val Loss: 0.2768 | Prec: 0.6144 | Rec: 0.7689 | F1: 0.6831\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.24it/s]\nEpoch 6/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 6/50:\n   Train Loss: 0.2691 | Prec: 0.6077 | Rec: 0.7966 | F1: 0.6895\n   Val Loss: 0.2864 | Prec: 0.6188 | Rec: 0.7395 | F1: 0.6738\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/50 [Train]: 100%|██████████| 14/14 [00:05<00:00,  2.36it/s]\nEpoch 7/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 7/50:\n   Train Loss: 0.2538 | Prec: 0.6299 | Rec: 0.8075 | F1: 0.7077\n   Val Loss: 0.2684 | Prec: 0.6435 | Rec: 0.7541 | F1: 0.6945\n💾 New best model saved! Val F1: 0.6945\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.31it/s]\nEpoch 8/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 8/50:\n   Train Loss: 0.2640 | Prec: 0.6068 | Rec: 0.8162 | F1: 0.6961\n   Val Loss: 0.2585 | Prec: 0.6265 | Rec: 0.8101 | F1: 0.7066\n💾 New best model saved! Val F1: 0.7066\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.23it/s]\nEpoch 9/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 9/50:\n   Train Loss: 0.2638 | Prec: 0.6247 | Rec: 0.7825 | F1: 0.6948\n   Val Loss: 0.2938 | Prec: 0.6291 | Rec: 0.7088 | F1: 0.6666\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.28it/s]\nEpoch 10/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 10/50:\n   Train Loss: 0.2682 | Prec: 0.6197 | Rec: 0.7823 | F1: 0.6916\n   Val Loss: 0.2797 | Prec: 0.7152 | Rec: 0.6950 | F1: 0.7050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.26it/s]\nEpoch 11/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 11/50:\n   Train Loss: 0.2505 | Prec: 0.6451 | Rec: 0.8106 | F1: 0.7184\n   Val Loss: 0.2692 | Prec: 0.6455 | Rec: 0.7585 | F1: 0.6974\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.30it/s]\nEpoch 12/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 12/50:\n   Train Loss: 0.2554 | Prec: 0.6317 | Rec: 0.8090 | F1: 0.7095\n   Val Loss: 0.2616 | Prec: 0.6264 | Rec: 0.7811 | F1: 0.6953\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.31it/s]\nEpoch 13/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 13/50:\n   Train Loss: 0.2631 | Prec: 0.5877 | Rec: 0.8302 | F1: 0.6882\n   Val Loss: 0.2873 | Prec: 0.6344 | Rec: 0.7302 | F1: 0.6789\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.33it/s]\nEpoch 14/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 14/50:\n   Train Loss: 0.2483 | Prec: 0.6077 | Rec: 0.8111 | F1: 0.6948\n   Val Loss: 0.2620 | Prec: 0.6832 | Rec: 0.7485 | F1: 0.7143\n💾 New best model saved! Val F1: 0.7143\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.30it/s]\nEpoch 15/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 15/50:\n   Train Loss: 0.2502 | Prec: 0.6612 | Rec: 0.7871 | F1: 0.7187\n   Val Loss: 0.2761 | Prec: 0.6743 | Rec: 0.7161 | F1: 0.6946\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/50 [Train]: 100%|██████████| 14/14 [00:05<00:00,  2.34it/s]\nEpoch 16/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 16/50:\n   Train Loss: 0.2577 | Prec: 0.6137 | Rec: 0.8223 | F1: 0.7029\n   Val Loss: 0.2677 | Prec: 0.6390 | Rec: 0.7614 | F1: 0.6948\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.22it/s]\nEpoch 17/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 17/50:\n   Train Loss: 0.2152 | Prec: 0.6955 | Rec: 0.8199 | F1: 0.7526\n   Val Loss: 0.2629 | Prec: 0.6706 | Rec: 0.7501 | F1: 0.7081\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.29it/s]\nEpoch 18/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 18/50:\n   Train Loss: 0.2379 | Prec: 0.6476 | Rec: 0.8302 | F1: 0.7276\n   Val Loss: 0.2631 | Prec: 0.6384 | Rec: 0.7731 | F1: 0.6993\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.29it/s]\nEpoch 19/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 19/50:\n   Train Loss: 0.2205 | Prec: 0.6953 | Rec: 0.8094 | F1: 0.7480\n   Val Loss: 0.2836 | Prec: 0.6915 | Rec: 0.6852 | F1: 0.6884\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.30it/s]\nEpoch 20/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 20/50:\n   Train Loss: 0.2225 | Prec: 0.6853 | Rec: 0.8300 | F1: 0.7507\n   Val Loss: 0.2612 | Prec: 0.6461 | Rec: 0.7900 | F1: 0.7108\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.19it/s]\nEpoch 21/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 21/50:\n   Train Loss: 0.2321 | Prec: 0.6395 | Rec: 0.8396 | F1: 0.7260\n   Val Loss: 0.2742 | Prec: 0.6514 | Rec: 0.7465 | F1: 0.6957\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/50 [Train]: 100%|██████████| 14/14 [00:05<00:00,  2.34it/s]\nEpoch 22/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 22/50:\n   Train Loss: 0.2269 | Prec: 0.6791 | Rec: 0.8157 | F1: 0.7411\n   Val Loss: 0.2765 | Prec: 0.6803 | Rec: 0.7100 | F1: 0.6948\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.32it/s]\nEpoch 23/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 23/50:\n   Train Loss: 0.2340 | Prec: 0.6808 | Rec: 0.8036 | F1: 0.7371\n   Val Loss: 0.2610 | Prec: 0.6928 | Rec: 0.7337 | F1: 0.7127\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/50 [Train]: 100%|██████████| 14/14 [00:05<00:00,  2.33it/s]\nEpoch 24/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 24/50:\n   Train Loss: 0.2048 | Prec: 0.7198 | Rec: 0.8327 | F1: 0.7722\n   Val Loss: 0.2517 | Prec: 0.6875 | Rec: 0.7542 | F1: 0.7193\n💾 New best model saved! Val F1: 0.7193\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.13it/s]\nEpoch 25/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 25/50:\n   Train Loss: 0.2068 | Prec: 0.6939 | Rec: 0.8436 | F1: 0.7615\n   Val Loss: 0.2511 | Prec: 0.6675 | Rec: 0.7784 | F1: 0.7187\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.18it/s]\nEpoch 26/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 26/50:\n   Train Loss: 0.2071 | Prec: 0.6955 | Rec: 0.8352 | F1: 0.7590\n   Val Loss: 0.2575 | Prec: 0.6710 | Rec: 0.7641 | F1: 0.7145\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.19it/s]\nEpoch 27/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 27/50:\n   Train Loss: 0.2029 | Prec: 0.6948 | Rec: 0.8488 | F1: 0.7641\n   Val Loss: 0.2448 | Prec: 0.6707 | Rec: 0.8037 | F1: 0.7312\n💾 New best model saved! Val F1: 0.7312\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.30it/s]\nEpoch 28/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 28/50:\n   Train Loss: 0.1965 | Prec: 0.7033 | Rec: 0.8595 | F1: 0.7736\n   Val Loss: 0.2481 | Prec: 0.6855 | Rec: 0.7762 | F1: 0.7281\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/50 [Train]: 100%|██████████| 14/14 [00:05<00:00,  2.33it/s]\nEpoch 29/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 29/50:\n   Train Loss: 0.2134 | Prec: 0.6673 | Rec: 0.8670 | F1: 0.7542\n   Val Loss: 0.2524 | Prec: 0.6830 | Rec: 0.7617 | F1: 0.7202\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.27it/s]\nEpoch 30/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 30/50:\n   Train Loss: 0.1959 | Prec: 0.7253 | Rec: 0.8412 | F1: 0.7790\n   Val Loss: 0.2666 | Prec: 0.6803 | Rec: 0.7353 | F1: 0.7067\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.26it/s]\nEpoch 31/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 31/50:\n   Train Loss: 0.2288 | Prec: 0.6611 | Rec: 0.8357 | F1: 0.7382\n   Val Loss: 0.2474 | Prec: 0.6775 | Rec: 0.7925 | F1: 0.7305\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.24it/s]\nEpoch 32/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 32/50:\n   Train Loss: 0.2124 | Prec: 0.7144 | Rec: 0.8340 | F1: 0.7696\n   Val Loss: 0.2705 | Prec: 0.6946 | Rec: 0.7154 | F1: 0.7048\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.32it/s]\nEpoch 33/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 33/50:\n   Train Loss: 0.2022 | Prec: 0.7091 | Rec: 0.8360 | F1: 0.7673\n   Val Loss: 0.2686 | Prec: 0.7007 | Rec: 0.7122 | F1: 0.7064\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.31it/s]\nEpoch 34/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 34/50:\n   Train Loss: 0.1996 | Prec: 0.7210 | Rec: 0.8415 | F1: 0.7766\n   Val Loss: 0.2481 | Prec: 0.6987 | Rec: 0.7613 | F1: 0.7287\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.29it/s]\nEpoch 35/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 35/50:\n   Train Loss: 0.2092 | Prec: 0.6769 | Rec: 0.8498 | F1: 0.7536\n   Val Loss: 0.2464 | Prec: 0.6885 | Rec: 0.7724 | F1: 0.7280\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.28it/s]\nEpoch 36/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 36/50:\n   Train Loss: 0.2040 | Prec: 0.7211 | Rec: 0.8285 | F1: 0.7711\n   Val Loss: 0.2523 | Prec: 0.6894 | Rec: 0.7568 | F1: 0.7215\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.19it/s]\nEpoch 37/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 37/50:\n   Train Loss: 0.2049 | Prec: 0.6904 | Rec: 0.8448 | F1: 0.7598\n   Val Loss: 0.2524 | Prec: 0.6872 | Rec: 0.7630 | F1: 0.7231\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.19it/s]\nEpoch 38/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 38/50:\n   Train Loss: 0.1948 | Prec: 0.7336 | Rec: 0.8378 | F1: 0.7823\n   Val Loss: 0.2625 | Prec: 0.7031 | Rec: 0.7252 | F1: 0.7140\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.27it/s]\nEpoch 39/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 39/50:\n   Train Loss: 0.2095 | Prec: 0.7139 | Rec: 0.8279 | F1: 0.7667\n   Val Loss: 0.2452 | Prec: 0.6772 | Rec: 0.7864 | F1: 0.7277\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.18it/s]\nEpoch 40/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  4.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 40/50:\n   Train Loss: 0.1928 | Prec: 0.7210 | Rec: 0.8520 | F1: 0.7810\n   Val Loss: 0.2401 | Prec: 0.6806 | Rec: 0.7964 | F1: 0.7339\n💾 New best model saved! Val F1: 0.7339\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.26it/s]\nEpoch 41/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 41/50:\n   Train Loss: 0.2399 | Prec: 0.6137 | Rec: 0.8559 | F1: 0.7149\n   Val Loss: 0.2521 | Prec: 0.6755 | Rec: 0.7692 | F1: 0.7193\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.30it/s]\nEpoch 42/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 42/50:\n   Train Loss: 0.2023 | Prec: 0.6904 | Rec: 0.8579 | F1: 0.7651\n   Val Loss: 0.2591 | Prec: 0.6749 | Rec: 0.7544 | F1: 0.7124\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.17it/s]\nEpoch 43/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 43/50:\n   Train Loss: 0.2194 | Prec: 0.7143 | Rec: 0.8009 | F1: 0.7551\n   Val Loss: 0.2689 | Prec: 0.6980 | Rec: 0.7152 | F1: 0.7065\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.28it/s]\nEpoch 44/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 44/50:\n   Train Loss: 0.2152 | Prec: 0.6991 | Rec: 0.8320 | F1: 0.7598\n   Val Loss: 0.2573 | Prec: 0.6962 | Rec: 0.7393 | F1: 0.7171\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.13it/s]\nEpoch 45/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 45/50:\n   Train Loss: 0.2037 | Prec: 0.7054 | Rec: 0.8417 | F1: 0.7675\n   Val Loss: 0.2531 | Prec: 0.6773 | Rec: 0.7699 | F1: 0.7206\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.18it/s]\nEpoch 46/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 46/50:\n   Train Loss: 0.1917 | Prec: 0.7362 | Rec: 0.8360 | F1: 0.7829\n   Val Loss: 0.2599 | Prec: 0.6915 | Rec: 0.7396 | F1: 0.7147\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.29it/s]\nEpoch 47/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 47/50:\n   Train Loss: 0.1894 | Prec: 0.7307 | Rec: 0.8451 | F1: 0.7838\n   Val Loss: 0.2577 | Prec: 0.6846 | Rec: 0.7486 | F1: 0.7151\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.15it/s]\nEpoch 48/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 48/50:\n   Train Loss: 0.1979 | Prec: 0.7264 | Rec: 0.8333 | F1: 0.7762\n   Val Loss: 0.2520 | Prec: 0.6882 | Rec: 0.7585 | F1: 0.7216\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.33it/s]\nEpoch 49/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 49/50:\n   Train Loss: 0.2020 | Prec: 0.7181 | Rec: 0.8408 | F1: 0.7746\n   Val Loss: 0.2472 | Prec: 0.6936 | Rec: 0.7658 | F1: 0.7279\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/50 [Train]: 100%|██████████| 14/14 [00:06<00:00,  2.19it/s]\nEpoch 50/50 [Val]: 100%|██████████| 4/4 [00:00<00:00,  5.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 50/50:\n   Train Loss: 0.1939 | Prec: 0.7415 | Rec: 0.8350 | F1: 0.7855\n   Val Loss: 0.2524 | Prec: 0.6861 | Rec: 0.7562 | F1: 0.7195\n\n🧪 Testing VM-UNet on test set...\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 4/4 [00:00<00:00,  4.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n🔍 Finding optimal threshold...\n   Threshold 0.3: Precision=0.7332, Recall=0.6404, F1=0.6837\n   Threshold 0.4: Precision=0.7583, Recall=0.6033, F1=0.6720\n   Threshold 0.5: Precision=0.7799, Recall=0.5689, F1=0.6579\n   Threshold 0.6: Precision=0.8010, Recall=0.5334, F1=0.6404\n   Threshold 0.7: Precision=0.8234, Recall=0.4928, F1=0.6166\n\n============================================================\n🎯 VM-UNet FINAL TEST RESULTS\n============================================================\nOptimal Threshold: 0.3\nTest Loss: 0.2774\nPrecision: 0.7332\nRecall:    0.6404\nAccuracy:  0.8965\nF1-Score:  0.6837\n============================================================\n\n💾 Final model saved!\n","output_type":"stream"}],"execution_count":60}]}